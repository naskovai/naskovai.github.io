<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Linear-Algebra on @naskovai</title>
    <link>http://localhost:1313/tags/linear-algebra/</link>
    <description>Recent content in Linear-Algebra on @naskovai</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Aug 2025 00:10:41 -0700</lastBuildDate><atom:link href="http://localhost:1313/tags/linear-algebra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Sandwich Framework for Understanding Linear Algebra</title>
      <link>http://localhost:1313/posts/sandwich-algebra/</link>
      <pubDate>Thu, 21 Aug 2025 00:10:41 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/sandwich-algebra/</guid>
      <description>Coordinate Translations, Scaling, and State Transitions - A unified approach to linear algebra decompositions</description>
      <content:encoded><![CDATA[<h1 id="the-sandwich-framework-for-understanding-linear-algebra-coordinate-translations-scaling-and-state-transitions">The Sandwich Framework for Understanding Linear Algebra: Coordinate Translations, Scaling, and State Transitions</h1>
<h2 id="core-philosophy-translate--transform--translate-back">Core Philosophy: Translate → Transform → Translate Back</h2>
<p>Every fundamental linear algebra operation follows the same pattern:</p>
<ol>
<li><strong>Translate</strong>: Change to coordinates where the problem becomes simple</li>
<li><strong>Transform</strong>: Apply scaling in those coordinates</li>
<li><strong>Translate Back</strong>: Change back to the original coordinate system</li>
</ol>
<h2 id="state-notation--matrix-types">State Notation &amp; Matrix Types</h2>
<p><strong>System State</strong>: What coordinate system the computation is currently operating in (e.g., Standard, Eigen, Singular)</p>
<p><strong>Operation Language</strong>: What coordinate system a matrix/operation is natively written in</p>
<p><strong>Matrix Types</strong> (the key insight!):</p>
<ul>
<li>🔄 <strong>Rotations/Reflections</strong>: Orthogonal matrices - preserve geometry, just change viewpoint</li>
<li>📏 <strong>Scaling</strong>: Diagonal matrices - stretch/shrink along axes, the &ldquo;actual work&rdquo;</li>
<li>🔀 <strong>General coordinate changes</strong>: Non-orthogonal matrices - might distort geometry</li>
<li>🎯 <strong>Projections</strong>: Combine rotation + truncation</li>
</ul>
<hr>
<h2 id="1-orthogonal-diagonalization-m--q-d-qt-symmetric-matrices">1) Orthogonal Diagonalization: $M = Q D Q^T$ (symmetric matrices)</h2>
<p><strong>What this is about</strong>: When you have a symmetric matrix (like a covariance matrix, Hessian, or quadratic form), this decomposition finds the natural &ldquo;principal axes&rdquo; where the matrix becomes diagonal. This is the cleanest possible decomposition because symmetric matrices have orthogonal eigenvectors, making all coordinate changes pure rotations.</p>
<p><strong>When to use</strong>: Symmetric matrices, quadratic forms, principal component analysis, optimization (finding principal axes of curvature).</p>
<p><strong>Legend for this case:</strong></p>
<p><em>Symbols:</em></p>
<ul>
<li>$M$: <strong>symmetric matrix written in Standard</strong></li>
<li>$Q$: <strong>orthogonal matrix of eigenvectors written in Standard</strong> (so $Q^T = Q^{-1}$)</li>
<li>$D$: <strong>diagonal matrix of eigenvalues written in Eigen</strong> (diagonal in eigenbasis)</li>
</ul>
<p><em>Operations:</em></p>
<ol>
<li>$Q^T$: <strong>Standard → Eigen</strong> (rotation - preserves geometry)</li>
<li>$D$: <strong>diagonal scaling in Eigen coords</strong></li>
<li>$Q$: <strong>Eigen → Standard</strong> (rotation - preserves geometry)</li>
</ol>
<p><strong>Geometric story</strong>: Rotate to natural axes, scale, rotate back.</p>
<p><strong>Operation Language</strong>: M, Q written in Standard; D written in Eigen<br>
<strong>System State Transitions</strong>:</p>
<ol>
<li>Start: Standard</li>
<li>$Q^T$: Standard → Eigen 🔄 rotation to eigen coords</li>
<li>$D$: acts in Eigen 📏 pure scaling in eigen coords</li>
<li>$Q$: Eigen → Standard 🔄 rotation back</li>
<li>End: Standard</li>
</ol>
<p><strong>Why it works</strong>: Symmetric matrices have orthogonal eigenvectors, so the coordinate change is a pure rotation.</p>
<hr>
<h2 id="2-matrix-in-eigen-language-3blue1brown-frame-p-1-m-p--d">2) &ldquo;Matrix in eigen language&rdquo; (3Blue1Brown frame): $P^{-1} M P = D$</h2>
<p><strong>What this is about</strong>: This is Grant Sanderson&rsquo;s (3Blue1Brown) perspective on eigendecomposition. Instead of applying a matrix to vectors, we&rsquo;re asking: &ldquo;What would this matrix look like if we changed our coordinate system to use eigenvectors as basis vectors?&rdquo; It&rsquo;s about relabeling the matrix itself rather than transforming vectors.</p>
<p><strong>When to use</strong>: When you want to understand what a linear transformation &ldquo;really does&rdquo; in its most natural coordinate system. Conceptual understanding of eigendecomposition.</p>
<p><strong>Legend for this case:</strong></p>
<p><em>Symbols:</em></p>
<ul>
<li>$M$: <strong>diagonalizable matrix written in Standard</strong></li>
<li>$P$: <strong>matrix of eigenvectors written in Standard</strong> (columns are eigenvectors, may not be orthogonal)</li>
<li>$D$: <strong>diagonal matrix of eigenvalues written in Eigen</strong></li>
</ul>
<p><em>Operations:</em></p>
<ol>
<li>$P$: <strong>converts Eigen inputs → Standard</strong> (so M can receive them)</li>
<li>$M$: <strong>acts in its native Standard language</strong></li>
<li>$P^{-1}$: <strong>converts Standard outputs → Eigen</strong> (so result is in Eigen)</li>
</ol>
<p><strong>Geometric story</strong>: Relabeling the operator itself, not applying to a vector.</p>
<p><strong>Operation Language</strong>: M, P written in Standard<br>
<strong>System Interpretation Changes</strong>:</p>
<ol>
<li>Start: System interprets operations in Standard</li>
<li>$P$: converts Eigen inputs → Standard 🔀 so M can receive them</li>
<li>$M$: acts in its native Standard language 🔀 processes in Standard</li>
<li>$P^{-1}$: converts Standard outputs → Eigen 🔀 result interpreted as Eigen</li>
<li>End: System sees M as acting Eigen → Eigen (becomes D)</li>
</ol>
<p><strong>Key insight</strong>: We&rsquo;re changing how we <em>interpret</em> the matrix&rsquo;s inputs/outputs, not transforming the system state.</p>
<hr>
<h2 id="3-general-diagonalization-m--p-d-p-1-non-symmetric">3) General Diagonalization: $M = P D P^{-1}$ (non-symmetric)</h2>
<p><strong>What this is about</strong>: When you have a non-symmetric matrix that&rsquo;s still diagonalizable, the eigenvectors are no longer orthogonal. This means the coordinate changes can distort geometry (stretching, shearing) rather than just rotating. It&rsquo;s the &ldquo;messy&rdquo; version of eigendecomposition where we lose the clean geometric properties.</p>
<p><strong>When to use</strong>: Non-symmetric matrices that are still diagonalizable, dynamic systems, Markov chains, some optimization problems.</p>
<p><strong>Legend for this case:</strong></p>
<p><em>Symbols:</em></p>
<ul>
<li>$M$: <strong>non-symmetric diagonalizable matrix written in Standard</strong></li>
<li>$P$: <strong>matrix of eigenvectors written in Standard</strong> (columns are eigenvectors, may not be orthogonal)</li>
<li>$D$: <strong>diagonal matrix of eigenvalues written in Eigen</strong></li>
</ul>
<p><em>Operations:</em></p>
<ol>
<li>$P^{-1}$: <strong>Standard → Eigen</strong> (may distort geometry)</li>
<li>$D$: <strong>diagonal scaling in Eigen coords</strong></li>
<li>$P$: <strong>Eigen → Standard</strong> (may distort geometry)</li>
</ol>
<p><strong>Geometric story</strong>: Change to skewed eigenbasis, scale, change back.</p>
<p><strong>Operation Language</strong>: M, P written in Standard; D written in Eigen<br>
<strong>System State Transitions</strong>:</p>
<ol>
<li>Start: Standard</li>
<li>$P^{-1}$: Standard → Eigen 🔀 general coordinate change (may distort)</li>
<li>$D$: acts in Eigen 📏 scaling in eigen coords</li>
<li>$P$: Eigen → Standard 🔀 change back</li>
<li>End: Standard</li>
</ol>
<p><strong>Key difference</strong>: $P^{-1}$ is typically NOT orthogonal, so this coordinate change can distort geometry.</p>
<hr>
<h2 id="4-qr-decomposition--projections-p_textproj--q-qt">4) QR Decomposition &amp; Projections: $P_{\text{proj}} = Q Q^T$</h2>
<p><strong>What this is about</strong>: This section focuses on the beautiful case where you already have orthonormal columns (matrix Q). Whether you got Q from QR decomposition of some original matrix A, or you started with orthonormal columns, projecting onto that subspace becomes elegantly simple: just $QQ^T$. This is the &ldquo;clean&rdquo; projection case.</p>
<p><strong>When to use</strong>: When you have orthonormal basis vectors for your subspace. After running QR decomposition and extracting Q. Projections in contexts where bases are already orthogonal.</p>
<p><strong>Legend for this case:</strong></p>
<p><em>Symbols:</em></p>
<ul>
<li>$Q$: <strong>orthogonal matrix written in Standard</strong> (columns are orthonormal basis for subspace, so $Q^T = Q^{-1}$)</li>
<li>$P_{\text{proj}}$: <strong>orthogonal projection operator written in Standard</strong></li>
</ul>
<p><em>Operations:</em></p>
<ol>
<li>$Q^T$: <strong>Standard → Subspace</strong> (rotation - preserves geometry)</li>
<li>$Q$: <strong>Subspace → Standard</strong> (rotation - preserves geometry)</li>
</ol>
<p><strong>Geometric story</strong>: Rotate to subspace coordinates, keep those components, rotate back.</p>
<p><strong>Operation Language</strong>: Q written in Standard<br>
<strong>System State Transitions</strong>:</p>
<ol>
<li>Start: Standard</li>
<li>$Q^T$: Standard → Subspace 🔄 rotation to subspace coords</li>
<li>(I): identity in Subspace 📏 identity = no scaling</li>
<li>$Q$: Subspace → Standard 🔄 rotation back</li>
<li>End: Standard</li>
</ol>
<p><strong>Insight</strong>: Orthogonal projection uses pure rotations - that&rsquo;s why it&rsquo;s geometrically clean.</p>
<hr>
<h2 id="5-general-projection-p--aat-a-1at">5) General Projection: $P = A(A^T A)^{-1}A^T$</h2>
<p><strong>What this is about</strong>: When you want to project onto a subspace but you only have non-orthogonal basis vectors (columns of A), you can&rsquo;t use the simple $AA^T$ formula. Instead, you need the pseudoinverse machinery to correct for the overlaps between non-orthogonal columns. This is the &ldquo;messy but general&rdquo; projection formula.</p>
<p><strong>When to use</strong>: Projecting onto subspaces when your basis isn&rsquo;t orthogonal, least squares with non-orthogonal regressors, data fitting to non-orthogonal function spaces.</p>
<p><strong>Legend for this case:</strong></p>
<p><em>Symbols:</em></p>
<ul>
<li>$A$: <strong>matrix with potentially non-orthogonal columns written in Standard</strong> (columns are standard coordinate vectors)</li>
<li>$A^T$: <strong>transpose of A, written in Standard</strong> (matrix entries stored normally)</li>
<li>$(A^T A)^{-1}$: <strong>inverse Gram matrix written in Standard</strong> (matrix entries stored normally)</li>
<li>$P$: <strong>projection operator written in Standard</strong> (result matrix stored normally)</li>
</ul>
<p><em>Operations:</em></p>
<ol>
<li>$A^T$: <strong>Standard → A_coords</strong> (extract coordinates w.r.t. A&rsquo;s columns)</li>
<li>$(A^T A)^{-1}$: <strong>A_coords → A_coords</strong> (correct for non-orthogonality in A_coords)</li>
<li>$A$: <strong>A_coords → Standard</strong> (reconstruct in Standard using A&rsquo;s columns)</li>
</ol>
<p><strong>Geometric story</strong>: Extract overlaps, correct for non-orthogonality, then reconstruct.</p>
<p><strong>Operation Language</strong>: A, $A^T$, $(A^T A)^{-1}$ all written in Standard<br>
<strong>System State Transitions</strong>:</p>
<ol>
<li>Start: Standard</li>
<li>$A^T$: Standard → A_coords 🔀 extract coordinates w.r.t. A&rsquo;s columns</li>
<li>$(A^T A)^{-1}$: A_coords → A_coords 🔀 correct for non-orthogonal columns</li>
<li>$A$: A_coords → Standard 🔀 reconstruct in standard coords</li>
<li>End: Standard</li>
</ol>
<p><strong>Why it&rsquo;s complex</strong>: When $A$ has non-orthogonal columns, we can&rsquo;t just use $AA^T$ like in the orthogonal case. Instead:</p>
<p><strong>Why it&rsquo;s complex</strong>: When $A$ has non-orthogonal columns, we can&rsquo;t just use $AA^T$ like in the orthogonal case. Instead:</p>
<ol>
<li><strong>$A^T$</strong>: Computes &ldquo;raw overlaps&rdquo; - how much the input vector overlaps with each column of $A$</li>
<li><strong>$(A^T A)^{-1}$</strong>: The <strong>Gram matrix correction</strong> - fixes the fact that A&rsquo;s columns aren&rsquo;t orthogonal</li>
<li><strong>$A$</strong>: Reconstructs the vector using only A&rsquo;s column space</li>
</ol>
<p><strong>The key insight</strong>: $(A^T A)^{-1}A^T$ together form the <strong>pseudoinverse</strong> of $A$, which is the generalization of inversion for non-square or non-orthogonal matrices.</p>
<hr>
<h2 id="6-svd-the-crown-jewel-a--u-sigma-vt">6) SVD: The Crown Jewel $A = U \Sigma V^T$</h2>
<p><strong>What this is about</strong>: SVD is the most general matrix decomposition - it works for ANY matrix (even non-square!). It finds the optimal coordinate systems for both input and output spaces simultaneously, revealing the fundamental structure of any linear transformation as &ldquo;rotate → scale → rotate&rdquo;. It&rsquo;s what eigendecomposition wishes it could be.</p>
<p><strong>When to use</strong>: Principal component analysis, data compression, image processing, collaborative filtering, any time you need the &ldquo;best&rdquo; low-rank approximation to data.</p>
<p><strong>Key insight: Singular values vs. Eigenvalues</strong></p>
<p>Before diving in, let&rsquo;s clarify a crucial distinction:</p>
<ul>
<li><strong>Eigenvalues</strong>: Only exist for square matrices, can be negative or complex, come from $Av = \lambda v$</li>
<li><strong>Singular values</strong>: Exist for ANY matrix (even non-square!), always non-negative real numbers, are the square roots of eigenvalues of $A^TA$ (or $AA^T$)</li>
</ul>
<p><strong>Why singular values are more universal:</strong></p>
<ul>
<li>Every matrix has singular values, but not every matrix has eigenvalues</li>
<li>Singular values tell you about the &ldquo;stretching factors&rdquo; of a matrix along its singular vector directions</li>
<li>The singular value decomposition works even when eigendecomposition fails (non-square or defective matrices)</li>
</ul>
<p><strong>The beautiful connection:</strong></p>
<ul>
<li>Singular values $\sigma_i$ are $\sqrt{\text{eigenvalues of } A^TA}$</li>
<li>The singular vectors (columns of $U$ and $V$) are the directions where $A$ achieves pure scaling</li>
<li>These are NOT the eigenvectors of $A$ itself (unless $A$ is symmetric), but rather the eigenvectors of $A^TA$ and $AA^T$</li>
</ul>
<p><strong>Legend for this case:</strong></p>
<p><em>Symbols:</em></p>
<ul>
<li>$A$: <strong>any matrix written in Standard</strong> (columns are standard coordinate vectors)</li>
<li>$U$: <strong>orthogonal matrix written in Standard</strong> (columns are standard coordinate vectors)</li>
<li>$\Sigma$: <strong>diagonal matrix bridging Right_singular → Left_singular</strong> (doesn&rsquo;t live in any single coordinate system)</li>
<li>$V$: <strong>orthogonal matrix written in Standard</strong> (columns are standard coordinate vectors)</li>
<li>$\Sigma^+$: <strong>pseudoinverse bridging Left_singular → Right_singular</strong> (inverse of Σ)</li>
</ul>
<p><em>Operations:</em></p>
<ol>
<li>$V^T$: <strong>Standard_input → Right_singular</strong> (rotation - preserves geometry)</li>
<li>$\Sigma$: <strong>Right_singular → Left_singular</strong> (diagonal scaling between DIFFERENT singular coordinate spaces)</li>
<li>$U$: <strong>Left_singular → Standard_output</strong> (rotation - preserves geometry)</li>
</ol>
<p><em>For pseudoinverse:</em></p>
<ol>
<li>$U^T$: <strong>Standard_output → Left_singular</strong> (rotation - preserves geometry)</li>
<li>$\Sigma^+$: <strong>Left_singular → Right_singular</strong> (inverse scaling between different coordinate spaces)</li>
<li>$V$: <strong>Right_singular → Standard_input</strong> (rotation - preserves geometry)</li>
</ol>
<p><strong>Geometric story</strong>: Optimal rotation in input space, pure scaling between spaces, optimal rotation in output space.</p>
<p><strong>Critical insight</strong>: Right_singular and Left_singular are <strong>different coordinate spaces</strong>:</p>
<ul>
<li><strong>Right_singular</strong>: Optimal basis for the input space (eigenvectors of $A^TA$)</li>
<li><strong>Left_singular</strong>: Optimal basis for the output space (eigenvectors of $AA^T$)</li>
<li><strong>SVD decomposition</strong>: Reveals how any linear transformation connects these optimal coordinate systems</li>
</ul>
<h3 id="6a-forward-y--a-x--u-sigma-vt-x">6a) Forward: $y = A x = U \Sigma V^T x$</h3>
<p><strong>What this is about</strong>: This is the basic SVD application - using the decomposition to apply the original matrix $A$ to a vector. It shows how any linear transformation can be broken down into three simple steps: rotate in input space, scale between spaces, rotate in output space.</p>
<p><strong>When to use</strong>: Understanding what a matrix &ldquo;really does&rdquo; geometrically, implementing matrix multiplication efficiently when you already have the SVD.</p>
<p><strong>Operation Language</strong>: A, U, V written in Standard; $\Sigma$ bridges coordinate systems<br>
<strong>System State Transitions (Input Space → Output Space)</strong>:</p>
<ol>
<li>Start: Standard_input</li>
<li>$V^T$: Standard_input → Right_singular 🔄 rotation in input space</li>
<li>$\Sigma$: Right_singular → Left_singular 📏 scaling between different singular spaces</li>
<li>$U$: Left_singular → Standard_output 🔄 rotation to output space</li>
<li>End: Standard_output</li>
</ol>
<h3 id="6b-pseudoinverse-x--a-y--v-sigma-ut-y">6b) Pseudoinverse: $x^+ = A^+ y = V \Sigma^+ U^T y$</h3>
<p><strong>What this is about</strong>: The pseudoinverse gives you the &ldquo;best possible inverse&rdquo; for any matrix, even non-square ones. It&rsquo;s the universal solution to &ldquo;given output $y$, what input $x$ most likely produced it?&rdquo; When $A$ is invertible, this gives the exact inverse. When it&rsquo;s not, it gives the least-squares best approximation.</p>
<p><strong>When to use</strong>: Solving linear systems that have no exact solution (overdetermined), data fitting, finding the &ldquo;closest&rdquo; solution to inconsistent systems, inverting non-square matrices.</p>
<p><strong>Operation Language</strong>: U, V written in Standard; $\Sigma^+$ bridges coordinate systems<br>
<strong>System State Transitions (Output Space → Input Space)</strong>:</p>
<ol>
<li>Start: Standard_output</li>
<li>$U^T$: Standard_output → Left_singular 🔄 rotation in output space</li>
<li>$\Sigma^+$: Left_singular → Right_singular 📏 inverse scaling between different spaces</li>
<li>$V$: Right_singular → Standard_input 🔄 rotation back to input space</li>
<li>End: Standard_input</li>
</ol>
<h3 id="6c-projections-via-svd">6c) Projections via SVD</h3>
<p><strong>What this is about</strong>: SVD gives you clean projections onto both the column space and row space of any matrix. Since $U$ and $V$ are already orthonormal, you get the beautiful $UU^T$ and $VV^T$ projection formulas without needing any pseudoinverse corrections. This connects SVD to the fundamental subspaces of linear algebra.</p>
<p><strong>When to use</strong>: Principal component analysis (projecting onto top singular directions), data compression, noise filtering, dimensionality reduction.</p>
<p><strong>Column space projection</strong>: $P_{\text{col}} = U_r U_r^T$.</p>
<ul>
<li>System transitions: Standard_output → Left_singular → Standard_output (keep first $r$ components)</li>
</ul>
<p><strong>Row space projection</strong>: $P_{\text{row}} = V_r V_r^T$ (acts in input space).</p>
<ul>
<li>System transitions: Standard_input → Right_singular → Standard_input (keep first $r$ components)</li>
</ul>
<h3 id="6d-whitening--preconditioning-at-a-12--v-sigma-1-vt">6d) Whitening / preconditioning: $(A^T A)^{-1/2} = V \Sigma^{-1} V^T$</h3>
<p><strong>What this is about</strong>: Whitening transforms your coordinate system so that the matrix $A^T A$ becomes the identity matrix. This &ldquo;undoes&rdquo; any stretching or correlation structure, making the space isotropic (same in all directions). It&rsquo;s like taking a stretched, tilted ellipse and turning it back into a perfect circle.</p>
<p><strong>When to use</strong>: Optimization (preconditioning gradient descent), machine learning (whitening data before training), signal processing (removing correlations), preparing data so that all dimensions are treated equally.</p>
<ul>
<li>$A^T A = V \Sigma^2 V^T$ (domain curvature)</li>
<li>$(A^T A)^{-1/2} = V \Sigma^{-1} V^T$
<ul>
<li>Apply to $g$: $V^T g$ → $\Sigma^{-1}$ → $V$.
Same <strong>translate → simple scale → translate back</strong> sandwich.</li>
</ul>
</li>
</ul>
<p><strong>Operation Language</strong>: V written in Standard; $\Sigma^{-1}$ acts within Right_singular coordinate system<br>
<strong>System State Transitions</strong>:</p>
<ol>
<li>Start: Standard</li>
<li>$V^T$: Standard → Right_singular 🔄 rotation to right singular coords</li>
<li>$\Sigma^{-1}$: Right_singular → Right_singular 📏 inverse scaling within right singular space</li>
<li>$V$: Right_singular → Standard 🔄 rotation back</li>
<li>End: Standard</li>
</ol>
<p><strong>Geometric meaning</strong>: Transform space so that $A^T A$ becomes identity (isotropic).</p>
<hr>
<h2 id="7-cholesky-decomposition-a--l-lt-positive-definite">7) Cholesky Decomposition: $A = L L^T$ (positive definite)</h2>
<p><strong>What this is about</strong>: When you have a positive definite symmetric matrix (like a covariance matrix or a &ldquo;nice&rdquo; quadratic form), you can factor it as the product of a lower triangular matrix with itself. Think of L as the &ldquo;square root&rdquo; of A. This is computationally efficient and numerically stable.</p>
<p><strong>When to use</strong>: Covariance matrices, solving systems with positive definite matrices, generating correlated random variables, optimization with quadratic objectives.</p>
<p><strong>Legend for this case:</strong></p>
<p><em>Symbols:</em></p>
<ul>
<li>$A$: <strong>positive definite symmetric matrix written in Standard</strong> (like covariance matrix)</li>
<li>$L$: <strong>lower triangular matrix written in Standard</strong> (the &ldquo;square root&rdquo; of A)</li>
</ul>
<p><em>Operations:</em></p>
<ol>
<li>$L^T$: <strong>Standard → Triangular</strong> (coordinate change - not rotation!)</li>
<li>$L$: <strong>Triangular → Standard</strong> (coordinate change - not rotation!)</li>
</ol>
<p><strong>Geometric story</strong>: Factor into &ldquo;square root&rdquo; operations.</p>
<p><strong>Operation Language</strong>: A, L written in Standard<br>
<strong>System State Transitions</strong>:</p>
<ol>
<li>$A = L L^T$ where L is lower triangular</li>
<li>Start: Standard</li>
<li>$L^T$: Standard → Triangular_coords 🔀 coordinate change (not rotation!)</li>
<li>$L$: Triangular_coords → Standard 🔀 coordinate change back</li>
<li>End: Standard</li>
</ol>
<p><strong>Special property</strong>: For correlation/covariance matrices, this gives the &ldquo;portfolio&rdquo; decomposition.</p>
<hr>
<h2 id="universal-principles">Universal Principles</h2>
<h3 id="-rotations-orthogonal-matrices">🔄 <strong>Rotations</strong> (Orthogonal matrices)</h3>
<ul>
<li><strong>Preserve</strong>: distances, angles, geometric relationships</li>
<li><strong>Change</strong>: only the coordinate system viewpoint</li>
<li><strong>Property</strong>: $Q^T Q = I$, $|Qx| = |x|$</li>
</ul>
<h3 id="-scaling-diagonal-matrices">📏 <strong>Scaling</strong> (Diagonal matrices)</h3>
<ul>
<li><strong>Preserve</strong>: coordinate directions (axes)</li>
<li><strong>Change</strong>: magnitudes along each axis</li>
<li><strong>Property</strong>: Acts independently on each coordinate</li>
</ul>
<h3 id="-general-coordinate-changes">🔀 <strong>General Coordinate Changes</strong></h3>
<ul>
<li><strong>May distort</strong>: distances, angles, geometric relationships</li>
<li><strong>Needed when</strong>: basis vectors aren&rsquo;t orthogonal</li>
</ul>
<h3 id="-projections">🎯 <strong>Projections</strong></h3>
<ul>
<li><strong>Combine</strong>: rotation to subspace + truncation + rotation back</li>
<li><strong>Orthogonal projections</strong>: pure rotations around truncation</li>
<li><strong>General projections</strong>: include metric corrections</li>
</ul>
<hr>
<h2 id="the-meta-pattern">The Meta-Pattern</h2>
<p><strong>Every decomposition answers</strong>: &ldquo;What&rsquo;s the simplest way to think about this operation?&rdquo;</p>
<ol>
<li><strong>Find the natural coordinate system</strong> (where the operation becomes diagonal/simple)</li>
<li><strong>Identify what the operation actually does</strong> (scale? project? rotate?)</li>
<li><strong>Package as</strong>: coordinate change + simple operation + coordinate change back</li>
</ol>
<p><strong>Reading rule</strong>: For any product $L S R$, read right-to-left:</p>
<ul>
<li>$R$: <strong>TRANSLATE</strong> - change system state to natural coordinates (may be rotation or general coordinate change)</li>
<li>$S$: <strong>TRANSFORM</strong> - scaling in those coordinates</li>
<li>$L$: <strong>TRANSLATE BACK</strong> - change system state back to original coordinates (inverse of $R$)</li>
</ul>
<p><strong>Key insight</strong>: The translate step (rightmost) may be orthogonal (rotation) or general (potentially distorting).</p>
<p>The magic is that this <strong>same pattern</strong> explains eigendecomposition, SVD, projections, least squares, whitening, and more. The differences are just in <em>which</em> coordinate systems are natural and <em>what</em> simple operation happens there.</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
